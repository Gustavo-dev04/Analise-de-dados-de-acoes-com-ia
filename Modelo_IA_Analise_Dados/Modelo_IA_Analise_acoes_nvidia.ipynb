{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOXQ8kiGZ2K24oy3DvSo10",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gustavo-dev04/Analise-de-dados-de-acoes-com-ia/blob/main/Modelo_IA_Analise_acoes_nvidia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìà Treinamento e Previs√£o de Dados Financeiros via Regress√£o Linear com PLN Avan√ßado  \n",
        "\n",
        "**Autor:** Gustavo Barros  \n",
        "**Data:** 06/04/2025  \n",
        "**Vers√£o:** 1.0  \n",
        "\n",
        "## **üìå Descri√ß√£o**  \n",
        "Este projeto utiliza **Processamento de Linguagem Natural (PLN)** avan√ßado e modelos de **regress√£o linear** para prever tend√™ncias financeiras (pre√ßos de ativos, indicadores econ√¥micos) a partir de:  \n",
        "- Dados estruturados (s√©ries temporais financeiras).  \n",
        "- Dados n√£o estruturados (not√≠cias, relat√≥rios, redes sociais).  \n",
        "\n",
        "**Objetivos:**  \n",
        "1. Coletar e processar dados multimodais (num√©ricos + textuais).  \n",
        "2. Treinar um modelo preditivo h√≠brido (PLN + regress√£o).  \n",
        "3. Automatizar insights para apoio a decis√µes.  \n",
        "\n",
        "## **‚öôÔ∏è M√©todos e Ferramentas**  \n",
        "- **Bibliotecas:** `scikit-learn` (regress√£o), `spaCy`/`NLTK` (PLN), `yfinance`/`Alpha Vantage` (dados financeiros).  \n",
        "- **IA Generativa:** Uso de *agents* (Deep Seek, Gemini) para otimiza√ß√£o de hiperpar√¢metros e documenta√ß√£o.  \n",
        "\n",
        "## **‚ö†Ô∏è Observa√ß√µes**  \n",
        "- Os dados podem conter ru√≠do (alta volatilidade financeira).  \n",
        "- O projeto foi acelerado com aux√≠lio de ferramentas de IA (GitHub Copilot, Deep Seek).\n",
        "\n",
        "## **üîù Cr√©ditos de Cursos Utilizados**\n",
        "1. Cursos de IA:\n",
        "- Machine Learning (Deeplearning.ai/Stanford Online)\n",
        "- AI Engineering (IBM)\n",
        "\n",
        "2. Curso de PNL:\n",
        "- Processamento Neural de Linguagem Natural em Portugu√™s I (Universidade de S√£o Paulo)\n",
        "\n",
        "3. Cursos de An√°lise de Dados:\n",
        "- Data Analytics (Google)\n",
        "- Advanced Data Analytics (Google)\n",
        "- Data Analytics Cloud (Google Cloud)\n",
        "\n",
        "4. Curso de Matem√°tica:\n",
        "- Mathematics for Machine Learning (Imperial College London)"
      ],
      "metadata": {
        "id": "YuRMgDaTwA_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**üìö Bibliotecas**\n",
        "As bibliotecas utilizadas para o processo:\n",
        "\n",
        "```\n",
        "# Downloads na interface do google colab\n",
        "# !python -m textblob.download_corpora  # Dados para an√°lise de sentimentos do TextBlob\n",
        "# !python -m spacy download en_core_web_sm  # Modelo de NLP em ingl√™s para Spacy\n",
        "# !python -m nltk.downloader vader_lexicon  # Lexicon para an√°lise de sentimentos VADER\n",
        "# !pip install yfinance feedparser textblob nltk  # Instala√ß√£o das bibliotecas principais\n",
        "\n",
        "# Bibliotecas b√°sicas para manipula√ß√£o de dados e visualiza√ß√£o\n",
        "import numpy as np  # Opera√ß√µes matem√°ticas eficientes\n",
        "import pandas as pd  # Manipula√ß√£o de dados em DataFrames\n",
        "import matplotlib.pyplot as plt  # Visualiza√ß√£o de dados\n",
        "import warnings  # Controle de avisos do sistema\n",
        "warnings.filterwarnings('ignore')  # Ignorar avisos n√£o cr√≠ticos\n",
        "import yfinance as yf  # Yahoo Finance API - coleta dados hist√≥ricos de a√ß√µes em tempo real\n",
        "\n",
        "# Machine Learning e Pr√©-processamento\n",
        "from sklearn.preprocessing import RobustScaler  # Normaliza√ß√£o robusta a outliers\n",
        "from sklearn.metrics import (  # M√©tricas de avalia√ß√£o de modelos\n",
        "    mean_absolute_error, #  Erro Absoluto Medio\n",
        "    mean_squared_error,  # Erro Quadratico Medio\n",
        "    r2_score             # Coeficiente de determina√ß√£o\n",
        ")\n",
        "\n",
        "# Web Scraping e Coleta de Not√≠cias\n",
        "import requests  # Requisi√ß√µes HTTP para coletar dados da web\n",
        "from bs4 import BeautifulSoup  # Parseamento de HTML/XML\n",
        "import feedparser  # Leitura de feeds RSS/Atom (not√≠cias financeiras)\n",
        "import re  # Express√µes regulares para limpeza de texto\n",
        "\n",
        "# Processamento de Linguagem Natural (NLP)\n",
        "from textblob import TextBlob  # An√°lise de sentimentos b√°sica\n",
        "from textblob.sentiments import NaiveBayesAnalyzer  # Classificador bayesiano para sentimentos\n",
        "import nltk  # Natural Language Toolkit\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer  # An√°lise de sentimentos espec√≠fica para finan√ßas\n",
        "\n",
        "# Controle de tempo\n",
        "import time  # Medi√ß√£o de tempo de execu√ß√£o/pausas\n",
        "from datetime import datetime, timedelta  # Manipula√ß√£o de datas\n",
        "\n",
        "# Configura√ß√µes\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['figure.figsize'] = (18, 9)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "mym6eTSHyoy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# PAR√ÇMETROS GLOBAIS DE CONFIGURA√á√ÉO\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "SYMBOL = 'NVDA'  # Ticker da a√ß√£o a ser analisada (NVIDIA)\n",
        "DAYS_TO_FETCH = 365 * 5  # Per√≠odo hist√≥rico (5 anos)\n",
        "TEST_SIZE = 0.2  # Propor√ß√£o dos dados para teste (20%)\n",
        "LEARNING_RATE = 0.05  # Taxa de aprendizado para otimiza√ß√£o do modelo\n",
        "EPOCHS = 10000  # N√∫mero m√°ximo de itera√ß√µes de treinamento\n",
        "VOLATILITY_WINDOW = 20  # Janela para c√°lculo de volatilidade (20 dias)\n",
        "REQUEST_TIMEOUT = 15  # Timeout para requisi√ß√µes web (segundos)\n",
        "MIN_NEWS_ARTICLES = 5  # M√≠nimo de not√≠cias para an√°lise v√°lida\n",
        "MAX_NEWS_ARTICLES = 10  # M√°ximo de not√≠cias a serem processadas\n",
        "```\n",
        "\n",
        "\n",
        "# CLASSE DE AN√ÅLISE DE NOT√çCIAS\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class GoogleNewsAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Inicializa o analisador de not√≠cias com configura√ß√µes padr√£o\"\"\"\n",
        "        self.session = requests.Session()  # Session para conex√µes HTTP persistentes\n",
        "        self.session.timeout = REQUEST_TIMEOUT  # Configura timeout\n",
        "        self.vader = SentimentIntensityAnalyzer()  # Analisador de sentimentos VADER\n",
        "\n",
        "        # Configura√ß√£o da URL do RSS do Google News para o s√≠mbolo\n",
        "        self.rss_url = f\"https://news.google.com/rss/search?q={SYMBOL}+stock&hl=en-US&gl=US&ceid=US:en\"\n",
        "\n",
        "        # Dicion√°rio de termos financeiros para an√°lise complementar\n",
        "        self.finance_terms = {\n",
        "            'positive': ['bullish', 'upgrade', 'beat', 'growth', 'raise', 'buy', 'outperform'],\n",
        "            'negative': ['bearish', 'downgrade', 'miss', 'cut', 'sell', 'underperform']\n",
        "        }\n",
        "\n",
        "    def fetch_google_news(self):\n",
        "        \"\"\"Coleta not√≠cias do Google News RSS\"\"\"\n",
        "        articles = []\n",
        "        try:\n",
        "            # Faz a requisi√ß√£o e parseia o feed RSS\n",
        "            response = self.session.get(self.rss_url, timeout=REQUEST_TIMEOUT)\n",
        "            feed = feedparser.parse(response.text)\n",
        "\n",
        "            # Processa cada entrada no feed\n",
        "            for entry in feed.entries[:MAX_NEWS_ARTICLES]:\n",
        "                # Tenta obter a data de publica√ß√£o (usa data atual como fallback)\n",
        "                pub_date = datetime.now()\n",
        "                if hasattr(entry, 'published_parsed'):\n",
        "                    pub_date = datetime(*entry.published_parsed[:6])\n",
        "\n",
        "                # Armazena informa√ß√µes relevantes da not√≠cia\n",
        "                articles.append({\n",
        "                    'title': entry.title,  # T√≠tulo da not√≠cia\n",
        "                    'source': entry.source.title if hasattr(entry, 'source') else 'Google News',  # Fonte\n",
        "                    'date': pub_date,  # Data de publica√ß√£o\n",
        "                    'url': entry.link  # URL original\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erro ao acessar Google News: {str(e)}\")\n",
        "\n",
        "        return articles\n",
        "\n",
        "    def _get_fallback_news(self):\n",
        "        \"\"\"Gera not√≠cias fict√≠cias como fallback quando h√° poucas not√≠cias reais\"\"\"\n",
        "        trends = ['high', 'growth', 'drop', 'volatile', 'steady']  # Termos de tend√™ncia\n",
        "        return [{\n",
        "            'title': f\"{SYMBOL} shows {np.random.choice(trends)} amid market {'gains' if np.random.random() > 0.5 else 'losses'}\",\n",
        "            'source': 'Market Trend',\n",
        "            'date': datetime.now(),\n",
        "            'url': ''\n",
        "        } for _ in range(MIN_NEWS_ARTICLES)]\n",
        "\n",
        "    def analyze_with_vader(self, text):\n",
        "        \"\"\"Analisa sentimento usando VADER (especializado para finan√ßas)\"\"\"\n",
        "        return self.vader.polarity_scores(text)['compound']  # Retorna score composto (-1 a 1)\n",
        "\n",
        "    def analyze_with_textblob(self, text):\n",
        "        \"\"\"Analisa sentimento usando TextBlob com NaiveBayes\"\"\"\n",
        "        analysis = TextBlob(text, analyzer=NaiveBayesAnalyzer())\n",
        "        return analysis.sentiment.p_pos - analysis.sentiment.p_neg  # Diferen√ßa de probabilidades\n",
        "\n",
        "    def analyze_finance_terms(self, text):\n",
        "        \"\"\"Analisa termos financeiros espec√≠ficos no texto\"\"\"\n",
        "        score = 0\n",
        "        text_lower = text.lower()\n",
        "        # Pontua termos positivos (+0.2 cada)\n",
        "        for term in self.finance_terms['positive']:\n",
        "            if term in text_lower:\n",
        "                score += 0.2\n",
        "        # Penaliza termos negativos (-0.3 cada)\n",
        "        for term in self.finance_terms['negative']:\n",
        "            if term in text_lower:\n",
        "                score -= 0.3\n",
        "        return score\n",
        "\n",
        "    def analyze_articles(self, articles):\n",
        "        \"\"\"Executa an√°lise completa do conjunto de not√≠cias\"\"\"\n",
        "        # Complementa com not√≠cias fict√≠cias se necess√°rio\n",
        "        if len(articles) < MIN_NEWS_ARTICLES:\n",
        "            print(f\"‚ö†Ô∏è Poucas not√≠cias ({len(articles)}), complementando com an√°lise de tend√™ncia...\")\n",
        "            articles.extend(self._get_fallback_news())\n",
        "\n",
        "        sentiment_results = []\n",
        "        print(\"\\nüì∞ An√°lise Avan√ßada de Not√≠cias (Google News):\")\n",
        "\n",
        "        # Processa cada artigo\n",
        "        for article in articles[:MAX_NEWS_ARTICLES]:\n",
        "            text = article['title']\n",
        "\n",
        "            # Combina m√∫ltiplos m√©todos de an√°lise\n",
        "            vader_score = self.analyze_with_vader(text)  # 50% peso\n",
        "            blob_score = self.analyze_with_textblob(text)  # 30% peso\n",
        "            finance_score = self.analyze_finance_terms(text)  # 20% peso\n",
        "\n",
        "            # Score combinado ponderado\n",
        "            combined_score = (vader_score * 0.5 + blob_score * 0.3 + finance_score * 0.2)\n",
        "\n",
        "            # Classifica√ß√£o por faixas de score\n",
        "            if combined_score > 0.2:\n",
        "                emoji = \"üöÄ\"\n",
        "                sentiment = \"FORTEMENTE POSITIVO\"\n",
        "            elif combined_score > 0.05:\n",
        "                emoji = \"üìà\"\n",
        "                sentiment = \"POSITIVO\"\n",
        "            elif combined_score < -0.2:\n",
        "                emoji = \"üí•\"\n",
        "                sentiment = \"FORTEMENTE NEGATIVO\"\n",
        "            elif combined_score < -0.05:\n",
        "                emoji = \"üìâ\"\n",
        "                sentiment = \"NEGATIVO\"\n",
        "            else:\n",
        "                emoji = \"‚ûñ\"\n",
        "                sentiment = \"NEUTRO\"\n",
        "\n",
        "            # Exibe resultado da an√°lise\n",
        "            print(f\"{emoji} [{sentiment}] {article['title'][:60]}... ({article['source']})\")\n",
        "            sentiment_results.append(combined_score)\n",
        "\n",
        "        # Calcula m√©dia de sentimentos\n",
        "        avg_sentiment = np.mean(sentiment_results) if sentiment_results else 0\n",
        "        print(f\"\\nüîç Sentimento M√©dio: {avg_sentiment:.2f} (Escala: -1 a 1)\")\n",
        "\n",
        "        # An√°lise de tend√™ncia temporal (se houver dados suficientes)\n",
        "        trend = 0\n",
        "        if len(articles) > 3 and all('date' in a for a in articles):\n",
        "            sorted_articles = sorted(articles, key=lambda x: x['date'])\n",
        "            early_scores = [self.analyze_with_vader(a['title']) for a in sorted_articles[:3]]\n",
        "            late_scores = [self.analyze_with_vader(a['title']) for a in sorted_articles[-3:]]\n",
        "            trend = np.mean(late_scores) - np.mean(early_scores)\n",
        "            print(f\"üìÖ Tend√™ncia Temporal: {'Melhorando' if trend > 0 else 'Piorando' if trend < 0 else 'Est√°vel'}\")\n",
        "\n",
        "        return {\n",
        "            'sentiment': avg_sentiment,  # Sentimento m√©dio\n",
        "            'trend': trend,  # Dire√ß√£o da tend√™ncia\n",
        "            'recent_impact': avg_sentiment * 1.5  # Impacto ponderado\n",
        "        }\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "# CLASSE PRINCIPAL DE PREVIS√ÉO\n",
        "\n",
        "\n",
        "```\n",
        "class EnhancedStockPredictor:\n",
        "    def __init__(self):\n",
        "        \"\"\"Inicializa o preditor com configura√ß√µes e modelos\"\"\"\n",
        "        self.learning_rate = LEARNING_RATE  # Taxa de aprendizado\n",
        "        self.epochs = EPOCHS  # M√°ximo de √©pocas\n",
        "        self.weights = None  # Pesos do modelo (ser√£o aprendidos)\n",
        "        self.bias = None  # Vi√©s do modelo (ser√° aprendido)\n",
        "        self.scaler = RobustScaler()  # Normalizador robusto a outliers\n",
        "        self.news_analyzer = GoogleNewsAnalyzer()  # Analisador de not√≠cias\n",
        "        # Colunas de features usadas no modelo\n",
        "        self.feature_cols = ['close', 'volume', 'ma_5', 'ma_20', 'volatility', 'momentum', 'news_sentiment', 'news_trend']\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"Prepara os dados para treinamento\"\"\"\n",
        "        try:\n",
        "            # Feature Engineering - cria novas vari√°veis explicativas\n",
        "            df['returns'] = df['close'].pct_change()  # Retornos di√°rios\n",
        "            df['ma_5'] = df['close'].rolling(5).mean()  # M√©dia m√≥vel 5 dias\n",
        "            df['ma_20'] = df['close'].rolling(20).mean()  # M√©dia m√≥vel 20 dias\n",
        "            df['volatility'] = df['returns'].rolling(VOLATILITY_WINDOW).std()  # Volatilidade\n",
        "            df['momentum'] = df['close'] - df['close'].shift(5)  # Momentum 5 dias\n",
        "\n",
        "            # Adiciona dados de sentimento de not√≠cias\n",
        "            articles = self.news_analyzer.fetch_google_news()\n",
        "            news_impact = self.news_analyzer.analyze_articles(articles)\n",
        "            df['news_sentiment'] = news_impact['sentiment']  # Sentimento m√©dio\n",
        "            df['news_trend'] = news_impact['trend']  # Tend√™ncia temporal\n",
        "\n",
        "            # Remove valores NA gerados pelos c√°lculos\n",
        "            df = df.dropna()\n",
        "\n",
        "            # Separa features (X) e target (y)\n",
        "            X = df[self.feature_cols]\n",
        "            y = df['close'].shift(-1).dropna()  # Pre√ßo do pr√≥ximo dia\n",
        "            X = X.iloc[:-1]  # Ajusta para coincidir com y\n",
        "\n",
        "            # Normaliza as features\n",
        "            X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "            # Divide em conjuntos de treino e teste\n",
        "            split_idx = int(len(X) * (1 - TEST_SIZE))\n",
        "            X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
        "            y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "            return X_train, X_test, y_train, y_test, df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro na prepara√ß√£o: {str(e)}\")\n",
        "            # Fallback: usa apenas features t√©cnicas se houver erro com not√≠cias\n",
        "            self.feature_cols = ['close', 'volume', 'ma_5', 'ma_20', 'volatility', 'momentum']\n",
        "            return self.prepare_data(df)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"Treina o modelo de regress√£o linear com gradiente descendente\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        # Inicializa pesos com pequenos valores aleat√≥rios\n",
        "        self.weights = np.random.randn(n_features) * 0.01\n",
        "        self.bias = 0\n",
        "        best_error = float('inf')  # Para early stopping\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Predi√ß√µes atuais\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "            # C√°lculo do erro com regulariza√ß√£o L2\n",
        "            error = np.mean((y_pred - y)**2) + 0.001 * np.sum(self.weights**2)\n",
        "\n",
        "            # Early stopping se n√£o houver melhoria significativa\n",
        "            if epoch > 100 and error > best_error * 0.999:\n",
        "                print(f\"‚úÖ Converg√™ncia na √©poca {epoch}\")\n",
        "                break\n",
        "\n",
        "            if error < best_error:\n",
        "                best_error = error\n",
        "\n",
        "            # C√°lculo dos gradientes\n",
        "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y)) + (0.002 * self.weights)\n",
        "            db = (1/n_samples) * np.sum(y_pred - y)          \n",
        "\n",
        "            # Atualiza√ß√£o dos par√¢metros\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            # Log peri√≥dico\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"√âpoca {epoch}: MSE = {error:.4f}\")\n",
        "\n",
        "        return best_error\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Faz previs√µes usando o modelo treinado\"\"\"\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Avalia o modelo com m√©tricas e gr√°ficos\"\"\"\n",
        "        y_pred = self.predict(X_test)\n",
        "\n",
        "        # C√°lculo das m√©tricas\n",
        "        mae = mean_absolute_error(y_test, y_pred)  # Erro absoluto m√©dio\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Raiz do erro quadr√°tico\n",
        "        r2 = r2_score(y_test, y_pred)  # Coeficiente de determina√ß√£o\n",
        "\n",
        "        # Plot dos resultados\n",
        "        plt.figure(figsize=(18, 9))\n",
        "        plt.plot(y_test.values, label='Real', linewidth=2)\n",
        "        plt.plot(y_pred, label='Previsto', linestyle='--')\n",
        "        plt.title(f'Desempenho do Modelo (MAE: ${mae:.2f}, R¬≤: {r2:.2%})')\n",
        "        plt.legend()\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "        return {'mae': mae, 'rmse': rmse, 'r2': r2}\n",
        "\n",
        "    def forecast(self, df, days=5):\n",
        "        \"\"\"Gera previs√µes para os pr√≥ximos dias\"\"\"\n",
        "        try:\n",
        "            forecasts = []\n",
        "            last_row = df.iloc[-1][self.feature_cols].values.reshape(1, -1)\n",
        "\n",
        "            for _ in range(days):\n",
        "                # Prepara dados para previs√£o\n",
        "                X_scaled = self.scaler.transform(last_row)\n",
        "                pred = self.predict(X_scaled)[0]\n",
        "                forecasts.append(pred)\n",
        "\n",
        "                # Atualiza features para pr√≥xima previs√£o\n",
        "                new_row = last_row.copy()\n",
        "                new_row[0][0] = pred  # Atualiza pre√ßo\n",
        "                # Atualiza m√©dias m√≥veis\n",
        "                new_row[0][2] = (new_row[0][0] + last_row[0][0] * 4) / 5  # MA5\n",
        "                new_row[0][3] = (new_row[0][0] + last_row[0][0] * 19) / 20  # MA20\n",
        "                last_row = new_row\n",
        "\n",
        "            # Prepara resultado\n",
        "            dates = [df.index[-1] + timedelta(days=i+1) for i in range(days)]\n",
        "            forecast_df = pd.DataFrame({'date': dates, 'forecast': forecasts})\n",
        "            forecast_df.set_index('date', inplace=True)\n",
        "\n",
        "            # Plot das previs√µes\n",
        "            plt.figure(figsize=(18, 9))\n",
        "            plt.plot(df['close'].iloc[-30:], label='Hist√≥rico', marker='o')\n",
        "            plt.plot(forecast_df['forecast'], label='Previs√£o', color='red', linestyle='--', marker='o')\n",
        "            plt.title(f'Previs√£o para {SYMBOL} - Pr√≥ximos {days} dias')\n",
        "            plt.legend()\n",
        "            plt.grid(alpha=0.3)\n",
        "            plt.show()\n",
        "\n",
        "            return forecast_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro na previs√£o: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "```\n",
        "\n",
        "# FUN√á√ÉO PRINCIPAL\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def main():\n",
        "    \"\"\"Fluxo principal de execu√ß√£o\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"=== AN√ÅLISE DE {SYMBOL} COM NLP NO GOOGLE NEWS ===\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    try:\n",
        "        # 1. Baixa dados hist√≥ricos do Yahoo Finance\n",
        "        print(\"üìä Baixando dados hist√≥ricos...\")\n",
        "        start_time = time.time()\n",
        "        df = yf.download(SYMBOL, period=f\"{DAYS_TO_FETCH//365}y\", progress=False)\n",
        "        df = df[['Close', 'Volume']]  # Mant√©m apenas pre√ßo e volume\n",
        "        df.columns = ['close', 'volume']  # Renomeia colunas\n",
        "        print(f\"‚úÖ Dados baixados - {len(df)} dias | √öltimo pre√ßo: ${df['close'].iloc[-1]:.2f}\")\n",
        "\n",
        "        # 2. Prepara dados com an√°lise de not√≠cias\n",
        "        print(\"\\nüß† Preparando dados com an√°lise de not√≠cias...\")\n",
        "        predictor = EnhancedStockPredictor()\n",
        "        X_train, X_test, y_train, y_test, df_processed = predictor.prepare_data(df)\n",
        "        print(f\"‚úÖ Dados preparados - {len(X_train)} treino, {len(X_test)} teste\")\n",
        "\n",
        "        # 3. Treina modelo\n",
        "        print(\"\\nüîÆ Treinando modelo...\")\n",
        "        train_start = time.time()\n",
        "        final_error = predictor.train(X_train, y_train)\n",
        "        print(f\"‚úÖ Treino conclu√≠do em {time.time()-train_start:.1f}s (MSE final: {final_error:.4f})\")\n",
        "\n",
        "        # 4. Avalia√ß√£o do modelo\n",
        "        print(\"\\nüìä Avaliando modelo...\")\n",
        "        metrics = predictor.evaluate(X_test, y_test)\n",
        "        print(f\"\\nüîç Resultados:\")\n",
        "        print(f\"- MAE: ${metrics['mae']:.2f}\")  # Erro m√©dio em d√≥lares\n",
        "        print(f\"- RMSE: ${metrics['rmse']:.2f}\")  # Erro quadr√°tico em d√≥lares\n",
        "        print(f\"- R¬≤: {metrics['r2']:.2%}\")  # Porcentagem de vari√¢ncia explicada\n",
        "\n",
        "        # 5. Gera previs√µes\n",
        "        print(\"\\nüîÆ Gerando previs√µes...\")\n",
        "        forecast = predictor.forecast(df_processed)\n",
        "        if forecast is not None:\n",
        "            print(\"\\nüìà Previs√µes para os pr√≥ximos dias:\")\n",
        "            print(forecast.round(2))  # Pre√ßos com 2 casas decimais\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n‚èπÔ∏è Execu√ß√£o interrompida pelo usu√°rio\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Erro: {str(e)}\")\n",
        "    finally:\n",
        "        print(f\"\\n‚è±Ô∏è Tempo total: {time.time()-start_time:.1f} segundos\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3D5BSmol--9h"
      }
    }
  ]
}

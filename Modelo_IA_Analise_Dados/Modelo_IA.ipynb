{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ITqHnLiMYAs1PuFDjLUe-BK_icWKcc4c",
      "authorship_tag": "ABX9TyM+W3pu+o/ScXps0giZk/sz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gustavo-dev04/Analise-de-dados-de-acoes-com-ia/blob/main/Modelo_IA_Analise_Dados/Modelo_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m nltk.downloader vader_lexicon\n",
        "!pip install yfinance feedparser textblob nltk\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import feedparser\n",
        "import time\n",
        "import warnings\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from textblob.sentiments import NaiveBayesAnalyzer\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['figure.figsize'] = (18, 9)\n",
        "\n",
        "SYMBOL = 'NVDA'\n",
        "DAYS_TO_FETCH = 365 * 5\n",
        "TEST_SIZE = 0.2\n",
        "LEARNING_RATE = 0.05\n",
        "EPOCHS = 50000\n",
        "VOLATILITY_WINDOW = 20\n",
        "REQUEST_TIMEOUT = 15\n",
        "MIN_NEWS_ARTICLES = 5\n",
        "MAX_NEWS_ARTICLES = 10\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "\n",
        "class GoogleNewsAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})\n",
        "        self.session.timeout = REQUEST_TIMEOUT\n",
        "        self.vader = SentimentIntensityAnalyzer()\n",
        "        self.rss_url = f\"https://news.google.com/rss/search?q={SYMBOL}+stock&hl=en-US&gl=US&ceid=US:en\"\n",
        "        self.finance_terms = {\n",
        "            'positive': ['bullish', 'upgrade', 'beat', 'growth', 'raise', 'buy', 'outperform'],\n",
        "            'negative': ['bearish', 'downgrade', 'miss', 'cut', 'sell', 'underperform']\n",
        "        }\n",
        "\n",
        "    def fetch_google_news(self):\n",
        "        articles = []\n",
        "        try:\n",
        "            response = self.session.get(self.rss_url, timeout=REQUEST_TIMEOUT)\n",
        "            response.raise_for_status()\n",
        "            feed = feedparser.parse(response.text)\n",
        "            for entry in feed.entries[:MAX_NEWS_ARTICLES]:\n",
        "                pub_date = datetime.now()\n",
        "                if hasattr(entry, 'published_parsed'):\n",
        "                    pub_date = datetime(*entry.published_parsed[:6])\n",
        "                articles.append({\n",
        "                    'title': entry.title,\n",
        "                    'source': entry.source.title if hasattr(entry, 'source') else 'Google News',\n",
        "                    'date': pub_date,\n",
        "                    'url': entry.link\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Erro ao acessar Google News: {str(e)}\")\n",
        "        return articles\n",
        "\n",
        "    def _get_fallback_news(self):\n",
        "        trends = ['high', 'growth', 'drop', 'volatile', 'steady']\n",
        "        return [{\n",
        "            'title': f\"{SYMBOL} shows {np.random.choice(trends)} amid market {'gains' if np.random.random() > 0.5 else 'losses'}\",\n",
        "            'source': 'Market Trend',\n",
        "            'date': datetime.now(),\n",
        "            'url': ''\n",
        "        } for _ in range(MIN_NEWS_ARTICLES)]\n",
        "\n",
        "    def analyze_with_vader(self, text):\n",
        "        return self.vader.polarity_scores(text)['compound']\n",
        "\n",
        "    def analyze_with_textblob(self, text):\n",
        "        analysis = TextBlob(text, analyzer=NaiveBayesAnalyzer())\n",
        "        return analysis.sentiment.p_pos - analysis.sentiment.p_neg\n",
        "\n",
        "    def analyze_finance_terms(self, text):\n",
        "        score = 0\n",
        "        text_lower = text.lower()\n",
        "        for term in self.finance_terms['positive']:\n",
        "            if term in text_lower:\n",
        "                score += 0.2\n",
        "        for term in self.finance_terms['negative']:\n",
        "            if term in text_lower:\n",
        "                score -= 0.3\n",
        "        return score\n",
        "\n",
        "    def analyze_articles(self, articles):\n",
        "        if len(articles) < MIN_NEWS_ARTICLES:\n",
        "            print(f\"âš ï¸ Poucas notÃ­cias ({len(articles)}), complementando com anÃ¡lise de tendÃªncia...\")\n",
        "            articles.extend(self._get_fallback_news())\n",
        "        sentiment_results = []\n",
        "        print(\"\\nðŸ“° AnÃ¡lise AvanÃ§ada de NotÃ­cias (Google News):\")\n",
        "        for article in articles[:MAX_NEWS_ARTICLES]:\n",
        "            text = article['title']\n",
        "            vader_score = self.analyze_with_vader(text)\n",
        "            blob_score = self.analyze_with_textblob(text)\n",
        "            finance_score = self.analyze_finance_terms(text)\n",
        "            combined_score = (vader_score * 0.5 + blob_score * 0.3 + finance_score * 0.2)\n",
        "            if combined_score > 0.2:\n",
        "                emoji = \"ðŸš€\"\n",
        "                sentiment = \"FORTEMENTE POSITIVO\"\n",
        "            elif combined_score > 0.05:\n",
        "                emoji = \"ðŸ“ˆ\"\n",
        "                sentiment = \"POSITIVO\"\n",
        "            elif combined_score < -0.2:\n",
        "                emoji = \"ðŸ’¥\"\n",
        "                sentiment = \"FORTEMENTE NEGATIVO\"\n",
        "            elif combined_score < -0.05:\n",
        "                emoji = \"ðŸ“‰\"\n",
        "                sentiment = \"NEGATIVO\"\n",
        "            else:\n",
        "                emoji = \"âž–\"\n",
        "                sentiment = \"NEUTRO\"\n",
        "            print(f\"{emoji} [{sentiment}] {article['title'][:60]}... ({article['source']})\")\n",
        "            sentiment_results.append(combined_score)\n",
        "        avg_sentiment = np.mean(sentiment_results) if sentiment_results else 0\n",
        "        print(f\"\\nðŸ” Sentimento MÃ©dio: {avg_sentiment:.2f} (Escala: -1 a 1)\")\n",
        "        trend = 0\n",
        "        if len(articles) > 3 and all('date' in a for a in articles):\n",
        "            sorted_articles = sorted(articles, key=lambda x: x['date'])\n",
        "            early_scores = [self.analyze_with_vader(a['title']) for a in sorted_articles[:3]]\n",
        "            late_scores = [self.analyze_with_vader(a['title']) for a in sorted_articles[-3:]]\n",
        "            trend = np.mean(late_scores) - np.mean(early_scores)\n",
        "            print(f\"ðŸ“… TendÃªncia Temporal: {'Melhorando' if trend > 0 else 'Piorando' if trend < 0 else 'EstÃ¡vel'}\")\n",
        "        return {\n",
        "            'sentiment': avg_sentiment,\n",
        "            'trend': trend,\n",
        "            'recent_impact': avg_sentiment * 1.5\n",
        "        }\n",
        "\n",
        "class EnhancedStockPredictor:\n",
        "    def __init__(self):\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.epochs = EPOCHS\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.scaler = RobustScaler()\n",
        "        self.news_analyzer = GoogleNewsAnalyzer()\n",
        "        self.feature_cols = ['close', 'volume', 'ma_5', 'ma_20', 'volatility', 'momentum', 'news_sentiment', 'news_trend']\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        try:\n",
        "            df['returns'] = df['close'].pct_change()\n",
        "            df['ma_5'] = df['close'].rolling(5).mean()\n",
        "            df['ma_20'] = df['close'].rolling(20).mean()\n",
        "            df['volatility'] = df['returns'].rolling(VOLATILITY_WINDOW).std()\n",
        "            df['momentum'] = df['close'] - df['close'].shift(5)\n",
        "            articles = self.news_analyzer.fetch_google_news()\n",
        "            news_impact = self.news_analyzer.analyze_articles(articles)\n",
        "            df['news_sentiment'] = news_impact['sentiment']\n",
        "            df['news_trend'] = news_impact['trend']\n",
        "            df = df.dropna()\n",
        "            X = df[self.feature_cols]\n",
        "            y = df['close'].shift(-1).dropna()\n",
        "            X = X.iloc[:-1]\n",
        "            X_scaled = self.scaler.fit_transform(X)\n",
        "            split_idx = int(len(X) * (1 - TEST_SIZE))\n",
        "            X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
        "            y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "            return X_train, X_test, y_train, y_test, df\n",
        "        except Exception as e:\n",
        "            print(f\"Erro na preparaÃ§Ã£o: {str(e)}\")\n",
        "            self.feature_cols = ['close', 'volume', 'ma_5', 'ma_20', 'volatility', 'momentum']\n",
        "            return self.prepare_data(df)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.random.randn(n_features) * 0.01\n",
        "        self.bias = 0\n",
        "        best_error = float('inf')\n",
        "        for epoch in range(self.epochs):\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "            error = np.mean((y_pred - y)**2) + 0.001 * np.sum(self.weights**2)\n",
        "            if epoch > 100 and error > best_error * 0.999:\n",
        "                print(f\"âœ… ConvergÃªncia na Ã©poca {epoch}\")\n",
        "                break\n",
        "            if error < best_error:\n",
        "                best_error = error\n",
        "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y)) + (0.002 * self.weights)\n",
        "            db = (1/n_samples) * np.sum(y_pred - y)\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "            if epoch % 500 == 0:\n",
        "                print(f\"Ã‰poca {epoch}: MSE = {error:.4f}\")\n",
        "        return best_error\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        y_pred = self.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        plt.figure(figsize=(18, 9))\n",
        "        plt.plot(y_test.values, label='Real', linewidth=2)\n",
        "        plt.plot(y_pred, label='Previsto', linestyle='--')\n",
        "        plt.title(f'Desempenho do Modelo (MAE: ${mae:.2f}, RÂ²: {r2:.2%})')\n",
        "        plt.legend()\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.show()\n",
        "        return {'mae': mae, 'rmse': rmse, 'r2': r2}\n",
        "\n",
        "    def forecast(self, df, days=5):\n",
        "        try:\n",
        "            forecasts = []\n",
        "            dates = []\n",
        "            last_row = df.iloc[-1][self.feature_cols].values.reshape(1, -1)\n",
        "            current_date = df.index[-1]\n",
        "            days_added = 0\n",
        "            while days_added < days:\n",
        "                current_date += timedelta(days=1)\n",
        "                if current_date.weekday() < 5:\n",
        "                    X_scaled = self.scaler.transform(last_row)\n",
        "                    pred = self.predict(X_scaled)[0]\n",
        "                    forecasts.append(pred)\n",
        "                    dates.append(current_date)\n",
        "                    days_added += 1\n",
        "                    new_row = last_row.copy()\n",
        "                    new_row[0][0] = pred\n",
        "                    new_row[0][2] = (new_row[0][0] + last_row[0][0] * 4) / 5\n",
        "                    new_row[0][3] = (new_row[0][0] + last_row[0][0] * 19) / 20\n",
        "                    last_row = new_row\n",
        "            forecast_df = pd.DataFrame({'date': dates, 'forecast': forecasts})\n",
        "            forecast_df.set_index('date', inplace=True)\n",
        "            plt.figure(figsize=(18, 9))\n",
        "            plt.plot(df['close'].iloc[-30:], label='HistÃ³rico', marker='o')\n",
        "            plt.plot(forecast_df['forecast'], label='PrevisÃ£o', color='red', linestyle='--', marker='o')\n",
        "            plt.title(f'PrevisÃ£o para {SYMBOL} - PrÃ³ximos {days} dias Ãºteis')\n",
        "            plt.legend()\n",
        "            plt.grid(alpha=0.3)\n",
        "            plt.show()\n",
        "            return forecast_df\n",
        "        except Exception as e:\n",
        "            print(f\"Erro na previsÃ£o: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "def main():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"=== ANÃLISE DE {SYMBOL} COM NLP NO GOOGLE NEWS ===\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "    try:\n",
        "        print(\"ðŸ“Š Baixando dados histÃ³ricos...\")\n",
        "        start_time = time.time()\n",
        "        df = yf.download(SYMBOL, period=f\"{DAYS_TO_FETCH//365}y\", progress=False)\n",
        "        df = df[['Close', 'Volume']]\n",
        "        df.columns = ['close', 'volume']\n",
        "        print(f\"âœ… Dados baixados - {len(df)} dias | Ãšltimo preÃ§o: ${df['close'].iloc[-1]:.2f}\")\n",
        "        print(\"\\nðŸ§  Preparando dados com anÃ¡lise de notÃ­cias...\")\n",
        "        predictor = EnhancedStockPredictor()\n",
        "        X_train, X_test, y_train, y_test, df_processed = predictor.prepare_data(df)\n",
        "        print(f\"âœ… Dados preparados - {len(X_train)} treino, {len(X_test)} teste\")\n",
        "        print(\"\\nðŸ”® Treinando modelo...\")\n",
        "        train_start = time.time()\n",
        "        final_error = predictor.train(X_train, y_train)\n",
        "        print(f\"âœ… Treino concluÃ­do em {time.time()-train_start:.1f}s (MSE final: {final_error:.4f})\")\n",
        "        print(\"\\nðŸ“Š Avaliando modelo...\")\n",
        "        metrics = predictor.evaluate(X_test, y_test)\n",
        "        print(f\"\\nðŸ” Resultados:\")\n",
        "        print(f\"- MAE: ${metrics['mae']:.2f}\")\n",
        "        print(f\"- RMSE: ${metrics['rmse']:.2f}\")\n",
        "        print(f\"- RÂ²: {metrics['r2']:.2%}\")\n",
        "        print(\"\\nðŸ”® Gerando previsÃµes...\")\n",
        "        forecast = predictor.forecast(df_processed)\n",
        "        if forecast is not None:\n",
        "            print(\"\\nðŸ“ˆ PrevisÃµes para os prÃ³ximos dias Ãºteis:\")\n",
        "            print(forecast.round(2))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nâ¹ï¸ ExecuÃ§Ã£o interrompida pelo usuÃ¡rio\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Erro: {str(e)}\")\n",
        "    finally:\n",
        "        print(f\"\\nâ±ï¸ Tempo total: {time.time()-start_time:.1f} segundos\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "PocZaXKaIWUz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}